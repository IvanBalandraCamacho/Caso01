# Caso01 â€” Sistema RAG con Multi-LLM y Streaming

Sistema avanzado de Retrieval-Augmented Generation (RAG) con soporte multi-proveedor de LLM, streaming en tiempo real, y exportaciÃ³n profesional de conversaciones.

## ğŸš€ CaracterÃ­sticas Principales

### ğŸ’¬ Chat Inteligente con RAG
- **Streaming en tiempo real**: Respuestas que se van escribiendo en vivo
- **Contexto de documentos**: BÃºsqueda semÃ¡ntica en vectores con Qdrant
- **Historial persistente**: Conversaciones guardadas en MySQL
- **Referencias a chunks**: Trazabilidad de fuentes usadas en cada respuesta

### ğŸ¤– Multi-LLM Provider Support
- **Google Gemini 2.0 Flash**: Modelo rÃ¡pido y eficiente (default)
- **OpenAI GPT-4.1 Nano**: Modelo compacto de alta calidad
- **Selector en UI**: Cambia de modelo durante la conversaciÃ³n
- **Precarga automÃ¡tica**: Ambos modelos listos al iniciar
- **Indicador visual**: Badge mostrando quÃ© modelo generÃ³ cada respuesta

### ğŸ“„ GestiÃ³n de Documentos
- **Formatos soportados**: PDF, DOCX, PPTX, XLSX, TXT, CSV
- **Procesamiento asÃ­ncrono**: Celery + Redis para chunking
- **Embeddings semÃ¡nticos**: Sentence Transformers (all-mpnet-base-v2)
- **Vector store**: Qdrant para bÃºsqueda eficiente

### ğŸ“¤ ExportaciÃ³n Profesional
- **Export a TXT**: Texto plano con formato Markdown convertido
- **Export a PDF**: Reportes profesionales con ReportLab
  - Estilos diferenciados por rol (Usuario/Asistente)
  - Sintaxis Markdown preservada (bold, italic, cÃ³digo)
  - Code blocks con fuente monoespaciada y fondo gris
  - Listas, headers y links formateados
- **Export por conversaciÃ³n**: Exporta solo el chat activo
- **Nombres descriptivos**: Archivos con timestamp Ãºnico

## ğŸ—ï¸ Arquitectura

### Backend (FastAPI)
```
backend/
â”œâ”€â”€ api/routes/          # Endpoints REST
â”‚   â””â”€â”€ workspaces.py    # Chat, documents, conversations, exports
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py        # ConfiguraciÃ³n centralizada
â”‚   â”œâ”€â”€ llm_service.py   # Factory pattern para LLM providers
â”‚   â””â”€â”€ providers/       # Implementaciones de LLM
â”‚       â”œâ”€â”€ llm_provider.py      # Abstract base class
â”‚       â”œâ”€â”€ gemini_provider.py   # Google Gemini implementation
â”‚       â””â”€â”€ openai_provider.py   # OpenAI GPT implementation
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ workspace.py     # Modelo de workspaces
â”‚   â”œâ”€â”€ document.py      # Modelo de documentos
â”‚   â”œâ”€â”€ conversation.py  # Modelos de conversaciones y mensajes
â”‚   â””â”€â”€ schemas.py       # Pydantic schemas
â””â”€â”€ processing/
    â”œâ”€â”€ parser.py        # Parseo de documentos
    â”œâ”€â”€ tasks.py         # Tareas Celery
    â””â”€â”€ vector_store.py  # IntegraciÃ³n con Qdrant
```

### Frontend (Next.js 16)
```
frontend/src/
â”œâ”€â”€ app/                 # App router (Next.js 16)
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ chat-area.tsx    # Chat UI con streaming
â”‚   â”œâ”€â”€ sidebar.tsx      # NavegaciÃ³n y selector de modelo
â”‚   â””â”€â”€ ui/              # shadcn/ui components
â”œâ”€â”€ context/
â”‚   â””â”€â”€ WorkspaceContext.tsx  # Estado global
â””â”€â”€ hooks/
    â””â”€â”€ useApi.ts        # API client con streaming
```

## ğŸ”§ Stack TecnolÃ³gico

- **Backend**: FastAPI, SQLAlchemy, Celery, Pydantic
- **Frontend**: Next.js 16 (Turbopack), TypeScript, React, Tailwind CSS, shadcn/ui
- **Base de Datos**: MySQL (metadata), Redis (cache/queue), Qdrant (vectores)
- **LLM**: Google Gemini 2.0 Flash, OpenAI GPT-4.1 Nano
- **Embeddings**: Sentence Transformers (all-mpnet-base-v2)
- **Documentos**: pypdf, python-docx, python-pptx, openpyxl, pdfplumber
- **PDF Generation**: ReportLab

## ğŸš€ Inicio RÃ¡pido

### 1. Configurar Variables de Entorno

```bash
cp backend/.env.example backend/.env
```

Edita `backend/.env` y configura:
- `GEMINI_API_KEY`: Tu API key de Google AI Studio
- `OPENAI_API_KEY`: Tu API key de OpenAI (opcional)
- `LLM_PROVIDER`: Proveedor por defecto ("gemini" o "openai")

### 2. Levantar con Docker Compose

```bash
docker-compose up -d --build
```

Esto iniciarÃ¡:
- **Backend** (FastAPI): http://localhost:8000
- **Frontend** (Next.js): http://localhost:3000
- **MySQL**: Puerto 3307
- **Redis**: Puerto 6380
- **Qdrant**: Puerto 6334
- **Celery Worker**: Procesamiento en background

### 3. Acceder a la AplicaciÃ³n

- ğŸŒ **Frontend**: http://localhost:3000
- ğŸ“š **API Docs**: http://localhost:8000/docs
- ğŸ” **Qdrant Dashboard**: http://localhost:6334/dashboard

## ğŸ“– Uso

### Crear Workspace y Subir Documentos

1. Crea un nuevo workspace desde el sidebar
2. Haz clic en "Upload Document"
3. Selecciona archivos (PDF, DOCX, etc.)
4. Espera a que se procesen (estado: pending â†’ processed)

### Chat con Multi-LLM

1. Selecciona un workspace con documentos procesados
2. Elige el modelo LLM desde el selector (Gemini 2.0 o GPT-4.1 Nano)
3. Escribe tu pregunta
4. Observa la respuesta en streaming
5. Cada respuesta muestra un badge con el modelo usado

### Exportar Conversaciones

1. Abre una conversaciÃ³n existente
2. Haz clic en "Export to TXT" o "Export to PDF"
3. El archivo descargarÃ¡ con:
   - Formato Markdown convertido a texto/PDF
   - Estilos profesionales en PDF
   - Timestamp y metadata de la conversaciÃ³n

## ğŸ”§ Comandos Ãštiles

```bash
# Ver logs en tiempo real
docker-compose logs -f backend
docker-compose logs -f celery_worker
docker-compose logs -f frontend

# Reiniciar servicios
docker-compose restart backend
docker-compose restart frontend

# Detener todo
docker-compose down

# Limpiar volÃºmenes (CUIDADO: elimina datos)
docker-compose down -v

# Reconstruir desde cero
docker-compose up -d --build --force-recreate
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Cambiar Modelo LLM por Defecto

En `backend/.env`:
```env
LLM_PROVIDER=openai  # o "gemini"
OPENAI_MODEL=gpt-4.1-nano-2025-04-14
GEMINI_MODEL=gemini-2.0-flash-exp
```

### Ajustar Chunk Size para RAG

En `backend/processing/parser.py`:
```python
CHUNK_SIZE = 1000  # Caracteres por chunk
CHUNK_OVERLAP = 200  # Overlap entre chunks
```

### Configurar CORS

En `backend/.env`:
```env
CORS_ALLOWED_ORIGINS=http://localhost:3000,https://tu-dominio.com
```

## ğŸ› Troubleshooting

### Backend no inicia
```bash
docker logs ia_backend
# Verificar que MySQL y Redis estÃ©n corriendo
docker-compose ps
```

### Frontend no conecta al backend
- Verifica que `NEXT_PUBLIC_API_URL` apunte a http://localhost:8000
- Revisa CORS en `backend/.env`

### Documentos no se procesan
```bash
docker logs ia_celery_worker
# Verificar conexiÃ³n a Redis
docker exec ia_backend redis-cli -h redis ping
```

### Streaming se repite o duplica
- Problema resuelto con `activeStreamRef` en chat-area.tsx
- Cada stream tiene ID Ãºnico para evitar updates de streams anteriores

### Export PDF corrupto
- Problema resuelto: ahora usa ReportLab correctamente
- Markdown se parsea y formatea con estilos profesionales

## ğŸ“š DocumentaciÃ³n Adicional

- **ENDPOINTS_API.md**: DocumentaciÃ³n completa de endpoints REST
- **API Docs**: http://localhost:8000/docs (Swagger UI automÃ¡tico)

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit: `git commit -m 'Add: nueva funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Abre un Pull Request

## ğŸ“ Changelog

### v2.1 (Noviembre 19, 2025)
- âœ… Multi-LLM provider support (Gemini + OpenAI)
- âœ… Streaming con prevenciÃ³n de duplicaciÃ³n
- âœ… Export TXT/PDF con formato Markdown
- âœ… Indicador visual de modelo activo
- âœ… Precarga de providers al iniciar
- âœ… Provider caching para mejor performance

### v2.0 (Noviembre 2025)
- Sistema RAG completo
- Procesamiento asÃ­ncrono con Celery
- Historial de conversaciones persistente

---

**Ãšltima actualizaciÃ³n**: Noviembre 19, 2025  
**VersiÃ³n**: 2.1  
**Autor**: IvanBalandraCamacho
